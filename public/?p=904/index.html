<!DOCTYPE html>
<html><head><script src="/chris_beeley_blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=chris_beeley_blog/livereload" data-no-instant defer></script>

  <title>Munging Patient Opinion data with R | Chris Beeley&#39;s blog</title>


  
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <link rel="icon" href="../favicon.ico">



  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>

  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>




  </head>
<body><nav class="navbar navbar-expand-lg navbar-light ">

  <a class="navbar-brand" href="../">Chris Beeley&#39;s blog</a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">

    <span class="navbar-toggler-icon"></span>

  </button>



  <div class="collapse navbar-collapse" id="navbarSupportedContent">

    <ul class="navbar-nav ml-auto">

      

      

        <li class="navbar-item ">

          <a class="nav-link" href="../chris_beeley_blog/posts/" title="posts">posts</a>

        </li>

      

        <li class="navbar-item ">

          <a class="nav-link" href="https://github.com/ChrisBeeley/chris_beeley_blog" title="GitHub">github</a>

        </li>

      

    </ul>

  </div>

</nav>
  <h1>Munging Patient Opinion data with R</h1>

  
  
  <time datetime="2017-03-10T11:31:23&#43;00:00">March 10, 2017</time>

  <p>I’ve written a post about using Patient Opinion’s new API, focusing on how to download your data using R but also perhaps useful to those using other languages as a gentle introduction, which can be found <a href="http://chrisbeeley.net/?p=886">here</a>.</p>
<p>This post focuses on the specific data operations which I perform in R in order to get the data nicely formatted and stored on our server. This may be of interest to those using R who want to do something very similar, much of this code will probably work in a similar context.</p>
<p>The code shown checks for the most recent Patient Opinion data every night and, if there is new data that is not yet on the database, downloads and processes it and uploads it to our own database. In the past I had downloaded the whole set every night and simply wiped the previous version, but this is not possible now we are starting to add our own data codes to the Patient Opinion data (enriching the metadata, for example by improving the match to our service tree all the way to team level where possible).</p>
<p>First things first, we’ll be using the lappend function, which I <a href="http://stackoverflow.com/questions/2436688/append-an-object-to-a-list-in-r-in-amortized-constant-time">stole from StackExchange</a> a very long time ago and use all the time. I’ve often thought of starting a campaign to get it included in base R but then I thought lots of people must have that idea about other functions and it probably really annoys the base R people, so I just define it whenever I need to use it. One day I will set up a proper <a href="http://www.onthelambda.com/2014/09/17/fun-with-rprofile-and-customizing-r-startup/">R starting environment</a> and include it in there, I use R on a lot of machines and so have always been frightened to open that particular can of worms.</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;

library(httr)
library(RMySQL)

### lappend function

lappend &lt;- function(lst, obj) {

  lst[[length(lst)+1]] &lt;- obj

  return(lst)
}
</code></pre><p>Now we store our api key somewhere where we can get it again and query our database to see the date of the most recent Patient Opinion story. We will issue a dbDisconnect(mydb) right at the end after we have finished the upload. Note that we add one to the date because we only want stories that are more recent than the database, and the API searches on dates after and <em>including</em> the given date.</p>
<p>We need to convert this date into a string like the following “01%2F08%2F2015” because this is the format the API accepts dates in. This date is the 1st August 2015.</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;

apiKey = &#34;SUBSCRIPTION_KEY qwertyuiop&#34;

# connect to database

theDriver &lt;- dbDriver(&#34;MySQL&#34;)

mydb = dbConnect(theDriver, user = &#34;myusername&#34;, password = &#34;mypassword&#34;,
  dbname = &#34;dbName&#34;, host = &#34;localhost&#34;)

dateQuery = as.Date(dbGetQuery(mydb, &#34;SELECT MAX(Date) FROM POFinal&#34;)[, 1]) + 1

dateFinal = paste0(substr(dateQuery, 6, 7), &#34;%2F&#34;, substr(dateQuery, 9, 10), &#34;%2F&#34;, substr(dateQuery, 1, 4))
</code></pre><p>Now it’s time to fetch the data from Patient Opinion and process it. We will produce an empty list and then add 100 stories to it (the maximum number the API will return from a single request), using a loop to take the next hundred, and the next hundred, until there are no more stories (it’s highly unlikely that there would be over 100 stories just in one day, obviously, but no harm in making the code cope with such an eventuality).</p>
<p>The API accepts a value of skip which allows you to select which story you want, in order of appearance, so this is set at 0, and then 100, and then 200, and so on within a loop to fetch all of the data.</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;

# produce empty list to lappend

opinionList = list()

# set skip at 0 and make continue TRUE until no stories are returned

skip = 0

continue = TRUE

while(continue){

  opinions = GET(paste0(&#34;https://www.patientopinion.org.uk/api/v2/opinions?take=100&amp;skip=&#34;,
  skip, &#34;&amp;submittedonafter=&#34;, dateFinal),
  add_headers(Authorization = apiKey))

  if(length(content(opinions)) == 0){ # if there are no stories then stop

  continue = FALSE
  }

  opinionList = c(opinionList, content(opinions)) # add the stories to the list
  # increase skip, and repeat

  skip = skip + 100

}
</code></pre><p>Having done this we test to see if there are any stories since yesterday. If there are, the length of opinionList will be greater than 0 (that is, length(opinionList) &gt; 0). If there are we extract the relevant bits we want from each list ready to build it into a dataframe</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;
# if there are no new stories just miss this entire bit out

if(length(opinionList) &gt; 0){

  keyID = lapply(opinionList, &#34;[[&#34;, &#34;id&#34;)
  title = lapply(opinionList, &#34;[[&#34;, &#34;title&#34;)
  story = lapply(opinionList, &#34;[[&#34;, &#34;body&#34;)
  date = lapply(opinionList, &#34;[[&#34;, &#34;dateOfPublication&#34;)
  criticality = lapply(opinionList, &#34;[[&#34;, &#34;criticality&#34;)
</code></pre><p>The service from which each story originates is slightly buried inside opinionList, and you’ll need to step through with lapply, extracting the $links element, and then take the object that results and step through it, taking the 5th list element and extracting the element named “id”. This actually makes more sense to explain in code:</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;
# location is inside $links

linkList = lapply(opinionList, &#34;[[&#34;, &#34;links&#34;)
location = lapply(linkList, function(x) x[[5]][[&#39;id&#39;]])

# Some general tidying up and we&#39;re ready to put in a dataframe

# there are null values in some of these, need converting to NA
# before they&#39;re put in the dataframe

date[sapply(date, is.null)] = NA
criticality[sapply(criticality, is.null)] = NA

finalData = data.frame(&#34;keyID&#34; = unlist(keyID),
  &#34;Title&#34; = unlist(title),
  &#34;PO&#34; = unlist(story),
  &#34;Date&#34; = as.Date(substr(unlist(date), 1, 10)),
  &#34;criticality&#34; = unlist(criticality),
  &#34;location&#34; = unlist(location),
  stringsAsFactors = FALSE
)
</code></pre><p>Now we have the data in a dataframe it’s time to match up the location codes that Patient Opinion use with the codes that we use internally. There’s nothing earth shattering in the code so I won’t laboriously explain it all, there may be a few lines in it that could help you with your own data.</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;
### match up NACS codes

POLookup = read.csv(&#34;poLookup.csv&#34;, stringsAsFactors = FALSE)

### some of the cases are inconsistent, make them all lower case

finalData$location = tolower(finalData$location)

POLookup$NACS = tolower(POLookup$NACS)

PO1 = merge(finalData, POLookup, by.x = &#34;location&#34;, by.y = &#34;NACS&#34;, all.x = TRUE)

### clean the data, removing HTML tags

PO1$PO = gsub(&#34;&lt;(.|\n)*?&gt;&#34;, &#34;&#34;, PO1$PO)

PO1$Date = as.Date(PO1$Date)

poQuarters = as.numeric(substr(quarters(PO1$Date), 2, 2))

poYears = as.numeric(format(PO1$Date, &#34;%Y&#34;))

PO1$Time = poQuarters + (poYears - 2009) * 4 - 1

### write PO to database

# all team codes must be present

PO1 = PO1[!is.na(PO1$TeamC), ]

# strip out line returns

PO1$PO = gsub(pattern = &#34;\r&#34;, replacement = &#34;&#34;, x = PO1$PO)
</code></pre><p>The following line is perhaps the most interesting. There are some smileys in our data, encoded in UTF-8, and I don’t know exactly why but RMySQL is not playing nicely with them. I’m sure there is a clever way to sort this problem but I have a lot of other work to do so I confess I just destroyed them as follows</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;
# strip out the UTF-8 smileys

PO1$PO = iconv(PO1$PO, &#34;ASCII&#34;, &#34;UTF-8&#34;, sub = &#34;&#34;)
</code></pre><p>And with all that done we can use dbWriteTable to upload the data to the database</p>
<pre tabindex="0"><code>&lt;pre class=&#34;brush: r; title: ; notranslate&#34; title=&#34;&#34;&gt;
# write to database

dbWriteTable(mydb, &#34;POFinal&#34;, PO1[, c(&#34;keyID&#34;, &#34;Title&#34;, &#34;PO&#34;, &#34;Date&#34;, &#34;Time&#34;,
             &#34;TeamC&#34;)], append = TRUE, row.names = FALSE)
}
</code></pre><p>That’s it! All done</p>

  

<footer class="footer mt-5">

  <div class="container">

    <table class="table table-sm">

      <caption>Hugo Variables for current page</caption>

      <tr><th>Name</th><td>Munging Patient Opinion data with R</td></tr>

      <tr><th>Kind</th><td>page</td></tr>

      <tr><th>Type</th><td>posts</td></tr>

      <tr><th>List Page</th><td>.Pages</td></tr>

      <tr><th>IsPage</th><td>true</td></tr>

      <tr><th>IsHome</th><td>true</td></tr>

      <tr><th>Next</th><td>/Users/NatsSpace/chris_beeley_blog_update/content/posts/2017-04-13-converting-a-grouped-plyrddply-to-dplyr.md</td></tr>

      <tr><th>Prev</th><td>/Users/NatsSpace/chris_beeley_blog_update/content/posts/2017-03-10-querying-the-patient-opinion-api-from-r.md</td></tr>

      <tr><th>Section</th><td>/Users/NatsSpace/chris_beeley_blog_update/content/posts/_index.md</td></tr>

    </table>

  </div>

</footer></body>
</html>
